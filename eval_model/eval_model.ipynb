{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uiEOHTqPV1Jt"
      },
      "source": [
        "# Скачиваем и устанавливаем всё необходимое"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLbSN09V6wnx"
      },
      "source": [
        "Удаляем ненужный sample_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fexntjO7-KPI"
      },
      "outputs": [],
      "source": [
        "!rm -rf sample_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2guIDT-36ylr"
      },
      "source": [
        "Клонируем наш репозиторий"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 a.py   "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtzqkpgvYSCc",
        "outputId": "1719947e-468c-494a-f3c6-c809fae40082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file 'a.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf logical_reasoning"
      ],
      "metadata": {
        "id": "KOV2cwEBZak2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTGeIr1nS2HA",
        "outputId": "d06a1224-363c-49d3-ae05-af65b2e1f364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'logical_reasoning'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (82/82), done.\u001b[K\n",
            "remote: Compressing objects: 100% (60/60), done.\u001b[K\n",
            "remote: Total 86 (delta 41), reused 47 (delta 18), pack-reused 4\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/HSE-projects/logical_reasoning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIEYHgSNy1BU"
      },
      "source": [
        "Удаляем torchtext так как в модели хотят версию торча 1.6, а она не поддерживается с новым torchtext, torchvision, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a10ZzMrKCuJk",
        "outputId": "16400f8b-0d0e-473f-9768-c9688e595533"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: torchtext 0.11.0\n",
            "Uninstalling torchtext-0.11.0:\n",
            "  Successfully uninstalled torchtext-0.11.0\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torchtext -y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RD1VwVL_60kW"
      },
      "source": [
        "Устанавливаем всё необходимое"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb5YAR651Ulk",
        "outputId": "b3216d88-6f25-4a5d-9770-d1f865868900"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/requirements.txt'\u001b[0m\n",
            "Collecting datasets\n",
            "  Downloading datasets-1.18.0-py3-none-any.whl (311 kB)\n",
            "\u001b[K     |████████████████████████████████| 311 kB 25.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 57.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n",
            "Collecting huggingface-hub<1.0.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 5.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 69.2 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 51.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.6)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 59.9 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.2.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (160 kB)\n",
            "\u001b[K     |████████████████████████████████| 160 kB 53.9 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 69.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.10)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, huggingface-hub, datasets\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-1.18.0 frozenlist-1.3.0 fsspec-2022.1.0 huggingface-hub-0.4.0 multidict-5.2.0 xxhash-2.0.2 yarl-1.7.2\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.7/dist-packages (3.6.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from gdown) (4.62.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->gdown) (1.24.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install -r logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/requirements.txt\n",
        "!pip install datasets\n",
        "!pip install gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tio9MmTO63fj"
      },
      "source": [
        "Скачиваем оптимальные параметры модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYgLkjR5YZGj",
        "outputId": "f70371ed-74b9-474e-a12f-918e456d7ff4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1AHMGgYFqcr3NmA-py6AMsO83w8FeNSJn\n",
            "To: /content/epoch=2-valid_loss=-0.2620-valid_acc_end=0.9223.ckpt\n",
            "100% 4.31G/4.31G [01:13<00:00, 58.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "# !gdown https://drive.google.com/uc?id=1bJVYekaOVJjI2woxqaqKMhWP0HLau105  # SNLI Base\n",
        "!gdown https://drive.google.com/uc?id=1AHMGgYFqcr3NmA-py6AMsO83w8FeNSJn  # SNLI Large"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One of them is absent\n",
        "!mv /content/epoch=4-valid_loss=-0.6472-valid_acc_end=0.9173.ckpt /content/snli_baseline.ckpt  # SNLI Base\n",
        "!mv /content/epoch=2-valid_loss=-0.2620-valid_acc_end=0.9223.ckpt /content/snli_baseline.ckpt  # SNLI Large"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_CIsYIrnQQMW",
        "outputId": "4da219f5-38e8-425f-acf9-7f641f06b8b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat '/content/epoch=4-valid_loss=-0.6472-valid_acc_end=0.9173.ckpt': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mxg_h1T67gn"
      },
      "source": [
        "Скачиваем модель roberta. Для этого надо установить [git-lfs](https://git-lfs.github.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfjVbL7G9CUi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad924c4e-8cc0-4e92-fd1b-e4076f8fff62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 1s (2,675 kB/s)\n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Error: Failed to call git rev-parse --git-dir --show-toplevel: \"fatal: not a git repository (or any of the parent directories): .git\\n\"\n",
            "Git LFS initialized.\n",
            "Cloning into 'roberta-large'...\n",
            "remote: Enumerating objects: 65, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (64/64), done.\u001b[K\n",
            "remote: Total 65 (delta 29), reused 0 (delta 0)\u001b[K\n",
            "Unpacking objects: 100% (65/65), done.\n",
            "tcmalloc: large alloc 1471086592 bytes == 0x556f94b9c000 @  0x7f255d8ac2a4 0x556f58f6cb9f 0x556f58f49cdb 0x556f58efee83 0x556f58ea318a 0x556f58ea3646 0x556f58ec03b1 0x556f58ec0949 0x556f58ec0e73 0x556f58f65892 0x556f58e07112 0x556f58deda15 0x556f58dee6d5 0x556f58ded6da 0x7f255cbf2bf7 0x556f58ded72a\n",
            "tcmalloc: large alloc 1471086592 bytes == 0x556f94b9c000 @  0x7f255d8ac2a4 0x556f58f6cb9f 0x556f58f49cdb 0x556f58efee83 0x556f58ea318a 0x556f58ea3646 0x556f58ec03b1 0x556f58ec0949 0x556f58ec0e73 0x556f58f65892 0x556f58e07112 0x556f58deda15 0x556f58dee6d5 0x556f58ded6da 0x7f255cbf2bf7 0x556f58ded72a\n",
            "tcmalloc: large alloc 1471086592 bytes == 0x556f94b9c000 @  0x7f255d8ac2a4 0x556f58f6cb9f 0x556f58f49cdb 0x556f58efee83 0x556f58ea318a 0x556f58ea3646 0x556f58ec03b1 0x556f58ec0949 0x556f58ec0e73 0x556f58f65892 0x556f58e07112 0x556f58deda15 0x556f58dee6d5 0x556f58ded6da 0x7f255cbf2bf7 0x556f58ded72a\n",
            "tcmalloc: large alloc 2206621696 bytes == 0x556fec68c000 @  0x7f255d8ac2a4 0x556f58f6cb9f 0x556f58f49cdb 0x556f58efee83 0x556f58ea318a 0x556f58ea3646 0x556f58ec03b1 0x556f58ec0949 0x556f58ec0e73 0x556f58f65892 0x556f58e07112 0x556f58deda15 0x556f58dee6d5 0x556f58ded6da 0x7f255cbf2bf7 0x556f58ded72a\n",
            "Filtering content: 100% (3/3), 4.17 GiB | 22.69 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install git-lfs\n",
        "!git lfs install\n",
        "# !git clone https://huggingface.co/roberta-base\n",
        "!git clone https://huggingface.co/roberta-large"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv8XGJNt7269"
      },
      "source": [
        "# Тестируем модель"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFnXxcKbdXp8"
      },
      "source": [
        "Меняем конфиг, чтобы был `num_labels` = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4F9kpFEdboT"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "config = json.load(open('roberta-large/config.json'))\n",
        "config['num_labels'] = 3\n",
        "with open('roberta-large/config.json', 'w') as f:\n",
        "    json.dump(config, f, indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EUBaXGzUZxxw",
        "outputId": "b6d3f6cb-1b4d-4917-df33-7bd6297a0e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.15.0-py3-none-any.whl (3.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 26.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 61.1 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.47 tokenizers-0.10.3 transformers-4.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from transformers.modeling_roberta import RobertaModel, RobertaConfig\n",
        "\n",
        "from dataset_utils.collate_functions import collate_to_max_length\n",
        "\n",
        "class ExplainableModel(nn.Module):\n",
        "    def __init__(self, bert_dir):\n",
        "        super().__init__()\n",
        "        self.bert_config = RobertaConfig.from_pretrained(bert_dir, output_hidden_states=False)\n",
        "        self.intermediate = RobertaModel.from_pretrained(bert_dir)\n",
        "        self.span_info_collect = SICModel(self.bert_config.hidden_size)\n",
        "        self.interpretation = InterpretationModel(self.bert_config.hidden_size)\n",
        "        self.output = nn.Linear(self.bert_config.hidden_size, self.bert_config.num_labels)\n",
        "\n",
        "    def forward(self, input_ids, start_indexs, end_indexs, span_masks):\n",
        "        # generate mask\n",
        "        attention_mask = (input_ids != 1).long()\n",
        "        # intermediate layer\n",
        "        hidden_states, first_token = self.intermediate(input_ids, attention_mask=attention_mask)  # output.shape = (bs, length, hidden_size)\n",
        "        # span info collecting layer(SIC)\n",
        "        h_ij = self.span_info_collect(hidden_states, start_indexs, end_indexs)\n",
        "        # interpretation layer\n",
        "        H, a_ij = self.interpretation(h_ij, span_masks)\n",
        "        # output layer\n",
        "        out = self.output(H)\n",
        "        return out, a_ij"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "-2xmTJlldW3r",
        "outputId": "8c2ad584-38b5-4a8b-807b-486b6ce35ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ae9c5645237f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodeling_roberta\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRobertaModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRobertaConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdataset_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollate_to_max_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'transformers.modeling_roberta'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as pl\n",
        "pl.metrics.F1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "Jqz00HMQaEZf",
        "outputId": "5934a2ab-7584-4fa5-e249-182c0e5faa60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e4a6bab2b0d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpytorch_lightning\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mF1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'pytorch_lightning.metrics' has no attribute 'F1'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3OFR9NV-Okx"
      },
      "source": [
        "## Тестируем модель на [SNLI](https://huggingface.co/datasets/snli)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Да, датасет есть на huggingface, но у авторов есть готовое решение с использованием SNLI, и оно его не использует. Поэтому я скачиваю SNLI с интеренета сам."
      ],
      "metadata": {
        "id": "YSLzooSTTQK-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример объекта из SNLI:\n",
        "\n",
        "| Столбец | Описание столбца | Значение |\n",
        "|-------- --|----------------|--------- |\n",
        "| sentence1 | Предпосылка | An old man with a package poses in front of an advertisement. |\n",
        "| sentence1_binary_parse | Дерево зависимостей для предпосылки | ( ( ( An ( old man ) ) ( with ( a package ) ) ) ( ( poses ( in ( front ( of ( an advertisement ) ) ) ) ) . ) ) |\n",
        "| sentence1_parse | Распарсенная предпосылка через Stanford PCFG Parser 3.5.2 | (ROOT (S (NP (NP (DT An) (JJ old) (NN man)) (PP (IN with) (NP (DT a) (NN package)))) (VP <br /> (VBZ poses) (PP (IN in) (NP (NP (NN front)) (PP (IN of) (NP (DT an) (NN advertisement)))))) (. .))) |\n",
        "| sentence2 | Гипотеза |  A man walks by an ad. |\n",
        "| sentence2_binary_parse | Дерево зависимостей для гипотезы | ( ( A man ) ( ( walks ( by ( an ad ) ) ) . ) ) |\n",
        "| sentence2_parse | Распарсенная гипотеза через Stanford PCFG Parser 3.5.2 | (ROOT (S (NP (DT A) (NN man)) (VP (VBZ walks) (PP (IN by) (NP (DT an) (NN ad)))) (. .))) |\n",
        "| annotator_labels | Ответы пяти mechanical turk workers (зарубежных толокеров) | [\"contradiction\", \"neutral\", \"contradiction\", \"contradiction\", \"contradiction\"] |\n",
        "| gold_label | Ответ на вопрос \"Как связаны эти предложения?\" | contradiction |\n",
        "| captionID | ??? |  4791890474.jpg#3 |\n",
        "| pairID | ??? |  4791890474.jpg#3r1c |"
      ],
      "metadata": {
        "id": "e_egR6gcigyI"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ9vIVwo7H17"
      },
      "source": [
        "Скачиваем и распаковываем SNLI. Также удаляем ненужные файлы."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c67j5YaPYc4H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmZZvftC67TH",
        "outputId": "8d3eb1d1-4225-4cba-b2f9-db6b5a6e24cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-22 20:05:07--  https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 94550081 (90M) [application/zip]\n",
            "Saving to: ‘snli_1.0.zip’\n",
            "\n",
            "snli_1.0.zip        100%[===================>]  90.17M  3.68MB/s    in 44s     \n",
            "\n",
            "2022-01-22 20:05:52 (2.03 MB/s) - ‘snli_1.0.zip’ saved [94550081/94550081]\n",
            "\n",
            "Archive:  snli_1.0.zip\n",
            "   creating: snli_1.0/\n",
            "  inflating: snli_1.0/.DS_Store      \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/snli_1.0/\n",
            "  inflating: __MACOSX/snli_1.0/._.DS_Store  \n",
            " extracting: snli_1.0/Icon           \n",
            "  inflating: __MACOSX/snli_1.0/._Icon  \n",
            "  inflating: snli_1.0/README.txt     \n",
            "  inflating: __MACOSX/snli_1.0/._README.txt  \n",
            "  inflating: snli_1.0/snli_1.0_dev.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_dev.txt  \n",
            "  inflating: snli_1.0/snli_1.0_test.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_test.txt  \n",
            "  inflating: snli_1.0/snli_1.0_train.jsonl  \n",
            "  inflating: snli_1.0/snli_1.0_train.txt  \n",
            "  inflating: __MACOSX/._snli_1.0     \n"
          ]
        }
      ],
      "source": [
        "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
        "!unzip snli_1.0.zip\n",
        "!rm -rf __MACOSX\n",
        "!rm snli_1.0.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq7iS4EySVge"
      },
      "source": [
        "Запускаем тестилку:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwNoHwe79jN0",
        "outputId": "99e71336-88ec-49f2-ed34-b118f8b3cbf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/snli_result’: File exists\n",
            "/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "Testing: 100% 983/983 [04:41<00:00,  4.25it/s]tensor(-0.7510, device='cuda:0') tensor(0.9221, device='cuda:0')\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The testing_epoch_end should not return anything as of 9.1.to log, use self.log(...) or self.write(...) directly in the LightningModule\n",
            "  warnings.warn(*args, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:45: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
            "Please use self.log(...) inside the lightningModule instead.\n",
            "\n",
            "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
            "# (inside LightningModule)\n",
            "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
            "  warnings.warn(*args, **kwargs)\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{'test_acc': tensor(0.9221, device='cuda:0'),\n",
            " 'test_loss': tensor(-0.7510, device='cuda:0'),\n",
            " 'val_loss': tensor(-0.7510, device='cuda:0')}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 983/983 [04:41<00:00,  3.49it/s]\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/snli_result\n",
        "%cd /content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
        "!python trainer.py \\\n",
        "--bert_path /content/roberta-large \\\n",
        "--data_dir /content/snli_1.0 \\\n",
        "--task snli \\\n",
        "--checkpoint_path '/content/snli_baseline.ckpt' \\\n",
        "--save_path /content/snli_result/ \\\n",
        "--gpus=0, \\\n",
        "--mode eval\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dfj49Ehh7lQs"
      },
      "source": [
        "## Тестируем модель на [SICK](https://huggingface.co/datasets/sick)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLI датасет + gold label + gold score - насколько предложения похожи от 1 до 5. В пары входили: очищенное от времён и прочего предложение + схожее, очищенное + противоположное, очищенное + обратное."
      ],
      "metadata": {
        "id": "JOGwc7uz122x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример объекта:\n",
        "\n",
        "| Столбец | Описание столбца | Значение |\n",
        "|---------|----|----------|\n",
        "| entailment_AB | Связь из А в В | A_contradicts_B |\n",
        "| entailment_BA | Связь из В в А | B_neutral_A |\n",
        "| id | ID объекта | 25 |\n",
        "| label | Gold label (0 - entailment, 1 - neutral, 2 - contradiction) | 1 |\n",
        "| relatedness_score | От 1 до 5, насколько предложения похожи | 2.799999952316284 |\n",
        "| sentence_A | Очищенное предложение А |  Nobody is riding the bicycle on one wheel |\n",
        "| sentence_A_dataset | Откуда достали А | FLICKR |\n",
        "| sentence_A_original | Исходное предложение А | A person rides their bicycle onto a rock and balances on one wheel |\n",
        "| sentence_B  | Очищенное предложение В | A person in a black jacket is doing tricks on a motorbike |\n",
        "| sentence_B_dataset  | Откуда достали В | FLICKR |\n",
        "| sentence_B_original  | Исходное предложение В | A person in a black jacket doing tricks on a motorbike. |"
      ],
      "metadata": {
        "id": "-YzBa39zmXIp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Поскольку модель не тестировали на SICK, то её придётся дообучать"
      ],
      "metadata": {
        "id": "pz1YaYQzI2ha"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUkLN7wT2OYf",
        "outputId": "2deba504-e4d6-4072-9d45-47884180453d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain'\n",
            "/content\n",
            "python3: can't open file 'trainer.py': [Errno 2] No such file or directory\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
        "!python trainer.py \\\n",
        "--bert_path /content/roberta-base \\\n",
        "--task sick \\\n",
        "--save_path /content/sick_ckpt \\\n",
        "--checkpoint_path '/content/snli_baseline.ckpt' \\\n",
        "--gpus=0,  \\\n",
        "--precision 16 \\\n",
        "--lr=2e-5 \\\n",
        "--batch_size=10 \\\n",
        "--lamb=1.0 \\\n",
        "--workers=1 \\\n",
        "--max_epoch=1\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9XnJIboego5",
        "outputId": "1a562ef0-8ed3-46b1-fde5-0cf28fa12efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
            "Traceback (most recent call last):\n",
            "  File \"trainer.py\", line 287, in <module>\n",
            "    main()\n",
            "  File \"trainer.py\", line 278, in main\n",
            "    evaluate(args)\n",
            "  File \"trainer.py\", line 265, in evaluate\n",
            "    checkpoint = torch.load(args.checkpoint_path, map_location=torch.device('cpu'))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 571, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 229, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 210, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/sick_ckpt/last.ckpt'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/sick_result\n",
        "%cd /content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
        "!python trainer.py \\\n",
        "--bert_path /content/roberta-large \\\n",
        "--task sick \\\n",
        "--checkpoint_path '/content/sick_ckpt/last.ckpt' \\\n",
        "--save_path /content/sick_result/ \\\n",
        "--gpus=0, \\\n",
        "--mode eval\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BVXdgyEZ9Z6B"
      },
      "source": [
        "## Тестируем модель на [ANLI](https://huggingface.co/datasets/anli)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Стоит сразу заметить, что ANLI создавал Facebook для ML контеста.\n",
        "\n",
        "Также я заметил, что все предпосылки очень длинные\n",
        "\n",
        "В этом контесте было три раунда, в связи с чем датасет разбит на 9 частей. Части i-го раунда имеют суффикс _ri.\n",
        "\n",
        "Я тестировал на 1-м раунде."
      ],
      "metadata": {
        "id": "YxGwI1VUzAro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример объекта:\n",
        "\n",
        "| Столбец | Описание столбца | Значение |\n",
        "|---------|------------------|----------|\n",
        "| premise | Предпосылка     | Charles William Ramsden (11 June 1904 – 16 February 1975) was an English footballer. His regular position was as a forward. <br /> He was born in Bucklow, Cheshire. He played for Rotherham Town, Stockport County, Manchester North End, and Manchester United. |\n",
        "| hypothesis | Гипотеза | Charles William Ramsden lived for over 70 years. |\n",
        "| label | Gold label (0 - entailment, 1 - neutral, 2 - contradiction) | 0 |\n",
        "| reason | Причина, почему такой gold label | Charles William Ramsden (11 June 1904 – 16 February 1975) was an English footballer. He lived more than 70 years.\n",
        "| uid | ID объекта | 8505cdf9-2f11-45a8-b6a3-02d6044971a7 |"
      ],
      "metadata": {
        "id": "QfLEY0H-xFnW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ANLI тоже нужно дообучать"
      ],
      "metadata": {
        "id": "pzaoYocGxDOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6mieflyOIEs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f10c0fa-879c-44c0-cf2a-969aad5bca2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: '/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain'\n",
            "/content\n",
            "python3: can't open file 'trainer.py': [Errno 2] No such file or directory\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd /content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
        "!python trainer.py \\\n",
        "--bert_path /content/roberta-large \\\n",
        "--task anli \\\n",
        "--save_path /content/anli_ckpt \\\n",
        "--checkpoint_path '/content/snli_baseline.ckpt' \\\n",
        "--gpus=0,  \\\n",
        "--precision 16 \\\n",
        "--lr=2e-4 \\\n",
        "--batch_size=10 \\\n",
        "--lamb=1.0 \\\n",
        "--workers=1 \\\n",
        "--max_epoch=1 \\\n",
        "--mode train\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIUyostR9gz1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "406bd919-c4a5-4deb-f380-9ac3ab61838f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
            "Traceback (most recent call last):\n",
            "  File \"trainer.py\", line 287, in <module>\n",
            "    main()\n",
            "  File \"trainer.py\", line 278, in main\n",
            "    evaluate(args)\n",
            "  File \"trainer.py\", line 265, in evaluate\n",
            "    checkpoint = torch.load(args.checkpoint_path, map_location=torch.device('cpu'))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 571, in load\n",
            "    with _open_file_like(f, 'rb') as opened_file:\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 229, in _open_file_like\n",
            "    return _open_file(name_or_buffer, mode)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/serialization.py\", line 210, in __init__\n",
            "    super(_open_file, self).__init__(open(name, mode))\n",
            "FileNotFoundError: [Errno 2] No such file or directory: '/content/anli_ckpt/last.ckpt'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!mkdir /content/anli_result\n",
        "%cd /content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
        "!python trainer.py \\\n",
        "--bert_path /content/roberta-large \\\n",
        "--task anli \\\n",
        "--checkpoint_path '/content/anli_ckpt/last.ckpt' \\\n",
        "--save_path /content/anli_result/ \\\n",
        "--gpus=0, \\\n",
        "--mode eval\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Тестируем модель на [Multi-NLI (MNLI)](https://huggingface.co/datasets/multi_nli)"
      ],
      "metadata": {
        "id": "mdRr02Gz1a31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Есть три набора данных:\n",
        "- train \n",
        "- validation_matched: предложения взяты из того же источника, откуда взяли train => они похожи на train\n",
        "- validation_mismatched: предложения взяли из другого источника => они сильно отличаются от train."
      ],
      "metadata": {
        "id": "4f2VSO5L34Tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример объекта:\n",
        "\n",
        "| Столбец | Описание столбца | Значение |\n",
        "|---------|------------------|----------|\n",
        "| premise | Предпосылка | harsh weather |\n",
        "| premise_binary_parse | Дерево зависимостей у предпосылки | ( harsh weather ) | \n",
        "| premise_parse | Распарсенная предпосылка через Stanford PCFG Parser 3.5.2 | (ROOT (NP (JJ harsh) (NN weather))) |\n",
        "| hypothesis | Гипотеза | good weather |\n",
        "| hypothesis_binary_parse | Дерево зависимостей у гипотезы | ( good ( weather . ) ) |\n",
        "| hypothesis_parse | Распарсенная гипотеза через Stanford PCFG Parser 3.5.2 | (ROOT (NP (JJ good) (NN weather) (. .))) |\n",
        "| label | Gold label (0 - entailment, 1 - neutral, 2 - contradiction) | 2 |\n",
        "| genre | Область, про которую написано в предпосылке и гипотезе | government |\n",
        "| promptID | ??? | 85161 |\n",
        "| pairID | ??? | 85161c |"
      ],
      "metadata": {
        "id": "O9NXbyqB1lK6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как обычно, дообучаем модель для MNLI:"
      ],
      "metadata": {
        "id": "St5NxSlCJ57u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
        "!python trainer.py \\\n",
        "--bert_path /content/roberta-large \\\n",
        "--task mnli \\\n",
        "--save_path /content/mnli_ckpt \\\n",
        "--checkpoint_path '/content/snli_baseline.ckpt' \\\n",
        "--gpus=0,  \\\n",
        "--precision 16 \\\n",
        "--lr=2e-5 \\\n",
        "--batch_size=10 \\\n",
        "--lamb=1.0 \\\n",
        "--workers=1 \\\n",
        "--max_epoch=1\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "15RrQ4DWHuSF",
        "outputId": "bc92e901-cbfc-4682-8952-7787250d7172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
            "Loading checkpoint\n",
            "Loaded checkpoint\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Using native 16bit precision.\n",
            "Missing logger folder: /content/mnli_ckpt/log\n",
            "initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "Traceback (most recent call last):\n",
            "  File \"trainer.py\", line 287, in <module>\n",
            "    main()\n",
            "  File \"trainer.py\", line 276, in main\n",
            "    train(args)\n",
            "  File \"trainer.py\", line 260, in train\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\", line 444, in fit\n",
            "    results = self.accelerator_backend.train()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/ddp_accelerator.py\", line 148, in train\n",
            "    results = self.ddp_train(process_idx=self.task_idx, model=model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/ddp_accelerator.py\", line 263, in ddp_train\n",
            "    self.setup_optimizers(model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/accelerators/accelerator.py\", line 198, in setup_optimizers\n",
            "    optimizers, lr_schedulers, optimizer_frequencies = self.trainer.init_optimizers(model)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/optimizers.py\", line 29, in init_optimizers\n",
            "    optim_conf = model.configure_optimizers()\n",
            "  File \"trainer.py\", line 76, in configure_optimizers\n",
            "    t_total = len(self.train_dataloader()) // self.args.accumulate_grad_batches * self.args.max_epochs\n",
            "  File \"trainer.py\", line 136, in train_dataloader\n",
            "    return self.get_dataloader(\"train\")\n",
            "  File \"trainer.py\", line 174, in get_dataloader\n",
            "    assert False, 'Unknown task found: ' + self.args.task\n",
            "AssertionError: Unknown task found: mnli\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/mnli_result\n",
        "%cd /content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
        "!python trainer.py \\\n",
        "--bert_path /content/roberta-larg \\\n",
        "--task mnli \\\n",
        "--checkpoint_path '/content/mnli_ckpt/last.ckpt' \\\n",
        "--save_path /content/mnli_result/ \\\n",
        "--gpus=0, \\\n",
        "--mode eval\n",
        "%cd /content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uZdQTn3Ljo-",
        "outputId": "c6d8fc9f-44f5-4f39-dace-ea9f51926ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 373, in get_config_dict\n",
            "    raise EnvironmentError\n",
            "OSError\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"trainer.py\", line 287, in <module>\n",
            "    main()\n",
            "  File \"trainer.py\", line 278, in main\n",
            "    evaluate(args)\n",
            "  File \"trainer.py\", line 264, in evaluate\n",
            "    model = ExplainNLP(args)\n",
            "  File \"trainer.py\", line 50, in __init__\n",
            "    self.model = ExplainableModel(self.bert_dir)\n",
            "  File \"/content/logical_reasoning/Self_Explaining_Structures_Improve_NLP_Models/explain/model.py\", line 21, in __init__\n",
            "    self.bert_config = RobertaConfig.from_pretrained(bert_dir, output_hidden_states=False)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 329, in from_pretrained\n",
            "    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/transformers/configuration_utils.py\", line 382, in get_config_dict\n",
            "    raise EnvironmentError(msg)\n",
            "OSError: Can't load config for '/content/roberta-larg'. Make sure that:\n",
            "\n",
            "- '/content/roberta-larg' is a correct model identifier listed on 'https://huggingface.co/models'\n",
            "\n",
            "- or '/content/roberta-larg' is the correct path to a directory containing a config.json file\n",
            "\n",
            "\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuX79y4pyABJ"
      },
      "source": [
        "# Итог"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eS166-FtysXD"
      },
      "source": [
        "\n",
        "| Датасет | test_acc | test_loss | val_loss | Место среди моделей | Где смотрел модели? |\n",
        "|---------|----------|-----------|----------|-----|----|\n",
        "| SNLI    | 0.9173   | -0.7510   | -0.7510  | 5 из 91 | [Papers With Code](https://paperswithcode.com/sota/natural-language-inference-on-snli) |\n",
        "| SICK   | 0.8849   | -0.7164   | -0.7164  | 2 из 12 | [Результаты в статье про NeuralLog](https://user-images.githubusercontent.com/29106126/144738746-74316acb-06b6-4f98-b451-5f6e0e5d9dc7.png) |\n",
        "| ANLI_r1 | 0.3880   |  0.6372   |  0.6372  | 7 из 8 | [Leaderboard на гитхабе датасета](https://github.com/facebookresearch/anli) |\n",
        "| MNLI | 0.8046 | -0.4656 | -0.4656 | 29 из 37 | [Papers With Code](https://paperswithcode.com/sota/natural-language-inference-on-multinli)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Копия блокнота \"logical-reasoning-dev.ipynb\"",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
